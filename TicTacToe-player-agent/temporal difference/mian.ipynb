{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from ai_temporalDifference   import *\n",
    "from environment import *\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "from multiprocessing import Process, Manager\n",
    "import multiprocessing\n",
    "import os\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = Manager()\n",
    "globalKnowlage = manager.dict()\n",
    "\n",
    "env = Board([4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulationArena(iterations , knowlageDict):\n",
    "    jimi = Agent(2,globalKnowlage)\n",
    "    sumi = Agent(1,globalKnowlage)\n",
    "    epsilon = 0.3\n",
    "    jimi.initialValue = sumi.initialValue = 1\n",
    "\n",
    "    for ep in range(iterations):\n",
    "        jimi.setEpsilon(epsilon)\n",
    "        sumi.setEpsilon(epsilon)\n",
    "\n",
    "        while True:\n",
    "        \n",
    "            action = jimi.deside(env.boardMap)\n",
    "            if action == (-1,-1):\n",
    "                print(\"ERRRRR\")\n",
    "            env.put(2, action)\n",
    "\n",
    "            if env.howIsTheWinner() == 2:\n",
    "                jimi.giveReward(1,True)\n",
    "                sumi.giveReward(-1,True)\n",
    "                # print('\\nWinner is 2')\n",
    "                jimi.score+=1\n",
    "                break\n",
    "            \n",
    "            elif env.isBoardFull():\n",
    "                jimi.giveReward(0,True)\n",
    "                sumi.giveReward(0,True)\n",
    "                break\n",
    "\n",
    "    ########################################\n",
    "            \n",
    "            action = sumi.deside(env.boardMap)\n",
    "            if action == (-1,-1):\n",
    "                print(\"ERRRRR\")\n",
    "            env.put(1, action)\n",
    "\n",
    "            if env.howIsTheWinner() == 1:\n",
    "                jimi.giveReward(-1,True)\n",
    "                sumi.giveReward(1,True)\n",
    "                # print('\\nWinner is 1')\n",
    "                sumi.score+=1\n",
    "                break\n",
    "\n",
    "            elif env.isBoardFull():\n",
    "                jimi.giveReward(0,True)\n",
    "                sumi.giveReward(0,True)\n",
    "                break\n",
    "\n",
    "        jimi.deside(env.boardMap)\n",
    "        sumi.deside(env.boardMap)\n",
    "        jimi.reset()\n",
    "        sumi.reset()\n",
    "        env.reset()\n",
    "\n",
    "        if ep % 2 == 0:\n",
    "            jimi.syncKnowlage()\n",
    "            sumi.syncKnowlage()\n",
    "\n",
    "        epsilon = epsilon*0.9 if epsilon> 0.1 else 0.1\n",
    "\n",
    "    return [jimi.score,sumi.score]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473.9375 313.8125] 25028\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(16) as pool:\n",
    "    scores = np.array(pool.starmap(simulationArena,[(1000,globalKnowlage)]*16))\n",
    "\n",
    "print(np.mean(scores,0),len(globalKnowlage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current after state value:  2.8137565068862154\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 2 0]]\n",
      "\n",
      "current after state value:  2.554109977844779\n",
      "[[1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [2 0 2 0]]\n",
      "\n",
      "current after state value:  2.943658937992108\n",
      "[[1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [2 1 2 0]]\n",
      "\n",
      "current after state value:  0\n",
      "[[1 1 2 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]\n",
      " [2 1 2 0]]\n",
      "\n",
      "current after state value:  0\n",
      "[[1 1 2 2]\n",
      " [0 1 0 0]\n",
      " [0 0 0 2]\n",
      " [2 1 2 0]]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "1,3 is taken before!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b1500827a9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misBoardFull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhumanTurn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhowIsTheWinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/TicTacToe-player-agent/environment.py\u001b[0m in \u001b[0;36mhumanTurn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your move: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhowIsTheWinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/TicTacToe-player-agent/environment.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, mark, pos)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboardMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{pos[0]+1},{pos[1]+1} is taken before!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboardMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: 1,3 is taken before!"
     ]
    }
   ],
   "source": [
    "jimi = Agent(2,globalKnowlage)\n",
    "\n",
    "jimi.setEpsilon(0.0)\n",
    "jimi.reset()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    running = True\n",
    "    while running:\n",
    "        r = 0\n",
    "\n",
    "\n",
    "        if env.isBoardFull():\n",
    "            break\n",
    "        \n",
    "        action = jimi.deside(env.boardMap)\n",
    "\n",
    "        if action == (-1,-1):\n",
    "            print('Errrrrr')\n",
    "            print(env.boardMap)\n",
    "\n",
    "        \n",
    "        env.put(2, action)\n",
    "\n",
    "############# printing\n",
    "        print()\n",
    "        f_hash = hashState(env.boardMap)\n",
    "        if f_hash in jimi.afterStateValues.keys():\n",
    "            value = jimi.afterStateValues[f_hash]\n",
    "        else:\n",
    "            value = 0\n",
    "\n",
    "        print(\"current after state value: \", value)\n",
    "\n",
    "############# printing\n",
    "\n",
    "        if env.howIsTheWinner() == 2:\n",
    "            r = 1\n",
    "            running = False\n",
    "            print('\\nWinner is 2')\n",
    "        elif not env.isBoardFull():\n",
    "\n",
    "            env.humanTurn()\n",
    "\n",
    "        if env.howIsTheWinner() == 1:\n",
    "            r = -1\n",
    "            running = False\n",
    "            print('\\nWinner is 1')\n",
    "\n",
    "        jimi.giveReward(r)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    jimi.reset()\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jimi.setEpsilon(0.1)\n",
    "jimi.reset()\n",
    "sumi.setEpsilon(0.1)\n",
    "sumi.reset()\n",
    "env.reset()\n",
    "\n",
    "tq = tqdm.trange(2000)\n",
    "\n",
    "for ep in tq:\n",
    "\n",
    "    running = True\n",
    "    \n",
    "\n",
    "    while running:\n",
    "        \n",
    "        action = jimi.deside(env.boardMap)\n",
    "        if action == (-1,-1):\n",
    "            print(\"ERRRRR\")\n",
    "        env.put(2, action)\n",
    "\n",
    "        if env.howIsTheWinner() == 2:\n",
    "            jimi.giveReward(1,True)\n",
    "            sumi.giveReward(-1,True)\n",
    "            # print('\\nWinner is 2')\n",
    "            jimi.score+=1\n",
    "            break\n",
    "        \n",
    "        elif env.isBoardFull():\n",
    "            jimi.giveReward(0,True)\n",
    "            sumi.giveReward(0,True)\n",
    "            break\n",
    "\n",
    "########################################\n",
    "        \n",
    "        action = sumi.deside(env.boardMap)\n",
    "        if action == (-1,-1):\n",
    "            print(\"ERRRRR\")\n",
    "        env.put(1, action)\n",
    "\n",
    "        if env.howIsTheWinner() == 1:\n",
    "            jimi.giveReward(-1,True)\n",
    "            sumi.giveReward(1,True)\n",
    "            # print('\\nWinner is 1')\n",
    "            sumi.score+=1\n",
    "            break\n",
    "\n",
    "        elif env.isBoardFull():\n",
    "            jimi.giveReward(0,True)\n",
    "            sumi.giveReward(0,True)\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "    jimi.deside(env.boardMap)\n",
    "    sumi.deside(env.boardMap)\n",
    "    jimi.learn()\n",
    "    sumi.learn()\n",
    "    jimi.reset()\n",
    "    sumi.reset()\n",
    "    env.reset()\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        tq.set_description(str(jimi.score)+' vs '+str(sumi.score))\n",
    "        jimi.score,sumi.score = 0,0\n",
    "    \n",
    "\n",
    "print(\"jimi: \",len(jimi.afterStateValues), 'score: ',jimi.score)\n",
    "print(\"sumi: \",len(sumi.afterStateValues), 'score: ',sumi.score)\n",
    "jimi.score,sumi.score = 0,0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8176438c5a17c06f3f3c8391faf90c450e02c98114053005859fe8b14d667a3b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}