{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "8176438c5a17c06f3f3c8391faf90c450e02c98114053005859fe8b14d667a3b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solves Ax = b linear equation system\n",
    "def jacobi_solver(A:np.ndarray,b:np.ndarray,initialValues = None ,acuraccy= 3):\n",
    "    if initialValues is None:\n",
    "        initialValues = np.zeros(b.shape)\n",
    "\n",
    "    oldx = x = initialValues\n",
    "    dInvers = np.linalg.inv(np.diag(A.diagonal()))\n",
    "    tm = A - np.diag(A.diagonal())\n",
    "\n",
    "\n",
    "    while(True):\n",
    "        x = dInvers@(b-tm@x)\n",
    "        if np.max(np.abs(np.round(x-oldx,acuraccy+1))) == 0:\n",
    "            break\n",
    "        oldx = x.copy()\n",
    "    \n",
    "    return x\n",
    "\n",
    "# generates bellman linear equation system's coefficients coresponding to given grid_map and policy\n",
    "def generate_bellman_eqSystem(grid_map,policy,terminal_states,epsilon)->tuple:\n",
    "    A = np.diag(np.ones(grid_map.size))\n",
    "    b = np.zeros(grid_map.size)\n",
    "    \n",
    "    for i in range(1,grid_map.shape[0]-1):\n",
    "        for j in range(1,grid_map.shape[1]-1):\n",
    "            if [i,j] in terminal_states:\n",
    "                continue\n",
    "\n",
    "\n",
    "            b[encodeIndex(i,j,grid_map)] = np.sum([policy[i,j,0]*grid_map[i+1,j] , policy[i,j,1]*grid_map[i-1,j] \n",
    "            , policy[i,j,2]*grid_map[i,j+1] , policy[i,j,3]*grid_map[i,j-1] ])\n",
    "\n",
    "\n",
    "\n",
    "            A[encodeIndex(i,j,grid_map), encodeIndex(i,j,grid_map)] = 1\n",
    "            A[encodeIndex(i,j,grid_map), encodeIndex(i+1,j,grid_map)] = -epsilon*policy[i,j,0]\n",
    "            A[encodeIndex(i,j,grid_map), encodeIndex(i-1,j,grid_map)] = -epsilon*policy[i,j,1]\n",
    "            A[encodeIndex(i,j,grid_map), encodeIndex(i,j+1,grid_map)] = -epsilon*policy[i,j,2]\n",
    "            A[encodeIndex(i,j,grid_map), encodeIndex(i,j-1,grid_map)] = -epsilon*policy[i,j,3]\n",
    "\n",
    "\n",
    "    return A,b\n",
    "\n",
    "\n",
    "def encodeIndex(i,j,grid_map)->int:\n",
    "    return i*grid_map.shape[1] + j\n",
    "\n",
    "# simulates a gready agent's actions based on given vValues\n",
    "def play_agent(vValues, startPos, terminalStates):\n",
    "    vValues[0,:] = -np.inf\n",
    "    vValues[-1,:] = -np.inf\n",
    "    vValues[:,0] = -np.inf\n",
    "    vValues[:,-1] = -np.inf\n",
    "    \n",
    "\n",
    "    res = np.zeros(vValues.shape)\n",
    "    pos = startPos\n",
    "    res[pos[0], pos[1]] = 8\n",
    "    actions = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    while (pos not in terminalStates):\n",
    "        \n",
    "\n",
    "        action = actions[np.argmax([ vValues[pos[0]+1,pos[1]], vValues[pos[0]-1,pos[1]]\n",
    "          , vValues[pos[0],pos[1]+1], vValues[pos[0],pos[1]-1] ])]\n",
    "        print(pos,action)\n",
    "        \n",
    "        pos[0],pos[1] = pos[0]+action[0], pos[1]+action[1]\n",
    "        res[pos[0], pos[1]] = 1\n",
    "        \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each cell's value determines the reward of each action which moves the agent to this cell.\n",
    "grid = np.array([\n",
    "    [ 0,  0,  0,  0 ,  0,  0, 0],\n",
    "    [ 0, 10, -1, -5 , -1, -1, 0],\n",
    "    [ 0, -1, -1, -1 , -1, -1, 0],\n",
    "    [ 0, -1, -1, -1 , -1, -1, 0],\n",
    "    [ 0, -1, -1, -1 , -1 ,-1, 0],\n",
    "    [ 0, -1, -1, -1 , -1, -1, 0],\n",
    "    [ 0,  0,  0,  0 ,  0,  0, 0]\n",
    "])\n",
    "\n",
    "\n",
    "# initalizing the policy\n",
    "policy = np.zeros([grid.shape[0], grid.shape[1], 4])\n",
    "\n",
    "for i in range(1,grid.shape[0]-1):\n",
    "    for j in range(1,grid.shape[1]-1):\n",
    "\n",
    "        counter =0\n",
    "        for a, action in enumerate([[1,0],[-1,0],[0,1],[0,-1]]):\n",
    "            sPrime = [i+action[0], j+action[1]]\n",
    "\n",
    "            if grid[sPrime[0],sPrime[1]] != 0:\n",
    "                policy[i,j,a] = 1\n",
    "                counter += 1\n",
    "\n",
    "        policy[i,j,:] /= counter\n",
    "\n",
    "policy= np.round(policy,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ],\n",
       "       [  0.  ,   0.  ,  -2.5 ,  -7.49, -10.35,  -9.97,   0.  ],\n",
       "       [  0.  ,  -0.62,  -5.38,  -9.04,  -9.62,  -9.59,   0.  ],\n",
       "       [  0.  ,  -5.58,  -7.3 ,  -8.78,  -9.33,  -9.36,   0.  ],\n",
       "       [  0.  ,  -7.55,  -8.27,  -8.91,  -9.24,  -9.27,   0.  ],\n",
       "       [  0.  ,  -8.23,  -8.53,  -8.88,  -9.13,  -9.28,   0.  ],\n",
       "       [  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ,   0.  ]])"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "A,b = generate_bellman_eqSystem(grid,policy,[[1,1]],0.9)\n",
    "vValues = jacobi_solver (A,b,acuraccy=10).reshape([7,7])\n",
    "np.round(vValues,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 5] [1, 0]\n[2, 5] [1, 0]\n[3, 5] [1, 0]\n[4, 5] [0, -1]\n[4, 4] [0, -1]\n[4, 3] [0, -1]\n[4, 2] [-1, 0]\n[3, 2] [-1, 0]\n[2, 2] [0, -1]\n[2, 1] [-1, 0]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 8., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "play_agent(vValues,startPos=[1,5],terminalStates=[[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}